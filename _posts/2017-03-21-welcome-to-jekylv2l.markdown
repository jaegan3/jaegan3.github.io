---
layout: blog
title: U Mad Bro
date: '2017-03-15 15:14:09 -0600'
categories: jekyll update devops
permalink: /blog/u-mad-bro
background: ../assets/img/blog-test-bg.jpg
published: true
---
Disaster. That word gets used a lot in our circles–it’s a trigger to the deepest FUD argument a vendor or colleague can make. A disaster can be defined in any number of ways: the number of customers impacted, revenue loss, or the number systems impacted. There are many metrics by which a disaster will be judged. For an on-call team however, the tale of a disaster is told in the minutes and the hours.

Much like a security breach, the reality of a systems disaster is not so much “Can it happen to us?” as “It hasn’t happened to us… yet.” Backup, restore, failover, resilience and scale are all terms that we focus on in disaster planning. We write Disaster Recovery Plans. We test failover patterns. We restore data from backups. But how much do we prepare our on-call teams for the long haul disaster event?

#Literally, the worst

![8008181336_c6457047ac_o.jpg]({{site.baseurl}}/assets/img/8008181336_c6457047ac_o.jpg)

Most Disaster Recovery Plans focus on what are perceived to be the highest risk and highest probability events. Highest risk? A comet strikes our office headquarters. Gosh that would be bad, right? The odds are pretty low, so let’s not really spend time thinking about that particular eventuality. So many disaster planning conversations end with “Well, if that actually happens, we’ll be screwed anyway.” You are honestly admitting that you won’t know how you’ll react in that eventuality.

That’s probably a reasonable response to the myriad of factors that will create and define your disaster. While you can’t game every potential disaster, you can be thoughtful beforehand about how your team will react and perform during this most critical time.

![840x577_blameless@2x.png]({{site.baseurl}}/assets/img/840x577_blameless@2x.png)



